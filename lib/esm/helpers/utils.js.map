{"version":3,"sources":["../../../src/helpers/utils.ts"],"names":["computeChecksumMd5","file","Promise","resolve","reject","chunkSize","spark","SparkMD5","ArrayBuffer","fileReader","FileReader","cursor","onerror","processChunk","chunk_start","chunk_end","Math","min","size","readAsArrayBuffer","slice","onload","e","append","target","result","end"],"mappings":";;;;;;;AAAA;;;;;;AAEO,SAASA,kBAAT,CAA4BC,IAA5B,EAAyD;AAC5D,SAAO,IAAIC,OAAJ,CAAY,CAACC,OAAD,EAAUC,MAAV,KAAqB;AACtC,UAAMC,SAAS,GAAG,OAAlB,CADsC,CACX;;AAC3B,UAAMC,KAAK,GAAG,IAAIC,QAAQ,CAACC,WAAb,EAAd;AACA,UAAMC,UAAU,GAAG,IAAIC,UAAJ,EAAnB;AAEA,QAAIC,MAAM,GAAG,CAAb,CALsC,CAKtB;;AAEhBF,IAAAA,UAAU,CAACG,OAAX,GAAqB,YAAiB;AACpCR,MAAAA,MAAM,CAAC,iDAAD,CAAN;AACD,KAFD,CAPsC,CAWtC;;;AACA,aAASS,YAAT,CAAsBC,WAAtB,EAAiD;AAC/C,YAAMC,SAAS,GAAGC,IAAI,CAACC,GAAL,CAAShB,IAAI,CAACiB,IAAd,EAAoBJ,WAAW,GAAGT,SAAlC,CAAlB;AAEAI,MAAAA,UAAU,CAACU,iBAAX,CAA6BlB,IAAI,CAACmB,KAAL,CAAWN,WAAX,EAAwBC,SAAxB,CAA7B;AACD,KAhBqC,CAkBtC;AACA;AACA;AACA;;;AACAN,IAAAA,UAAU,CAACY,MAAX,GAAoB,UAASC,CAAT,EAAuB;AACzChB,MAAAA,KAAK,CAACiB,MAAN,CAAaD,CAAC,CAACE,MAAF,CAASC,MAAtB,EADyC,CACV;;AAC/Bd,MAAAA,MAAM,IAAIN,SAAV,CAFyC,CAEpB;;AAErB,UAAIM,MAAM,GAAGV,IAAI,CAACiB,IAAlB,EAAwB;AACtB;AACAL,QAAAA,YAAY,CAACF,MAAD,CAAZ;AACD,OAHD,MAGO;AACL;AACA;AACA;AACA;AAEA;AACA;AACAR,QAAAA,OAAO,CAACG,KAAK,CAACoB,GAAN,EAAD,CAAP;AACD;AACF,KAjBD;;AAmBAb,IAAAA,YAAY,CAAC,CAAD,CAAZ;AACD,GA1CM,CAAP;AA2CD","sourcesContent":["import * as SparkMD5 from 'spark-md5';\n\nexport function computeChecksumMd5(file: File): Promise<string> {\n    return new Promise((resolve, reject) => {\n      const chunkSize = 5242880; // Read in chunks of 5MB\n      const spark = new SparkMD5.ArrayBuffer();\n      const fileReader = new FileReader();\n  \n      let cursor = 0; // current cursor in file\n  \n      fileReader.onerror = function(): void {\n        reject('MD5 computation failed - error reading the file');\n      };\n  \n      // read chunk starting at `cursor` into memory\n      function processChunk(chunk_start: number): void {\n        const chunk_end = Math.min(file.size, chunk_start + chunkSize);\n        \n        fileReader.readAsArrayBuffer(file.slice(chunk_start, chunk_end));\n      }\n  \n      // when it's available in memory, process it\n      // If using TS >= 3.6, you can use `FileReaderProgressEvent` type instead \n      // of `any` for `e` variable, otherwise stick with `any`\n      // See https://github.com/Microsoft/TypeScript/issues/25510\n      fileReader.onload = function(e: any): void {\n        spark.append(e.target.result); // Accumulate chunk to md5 computation\n        cursor += chunkSize; // Move past this chunk\n  \n        if (cursor < file.size) {\n          // Enqueue next chunk to be accumulated\n          processChunk(cursor);\n        } else {\n          // Computation ended, last chunk has been processed. Return as Promise value.\n          // This returns the base64 encoded md5 hash, which is what\n          // Rails ActiveStorage or cloud services expect\n          // resolve(btoa(spark.end(true)));\n  \n          // If you prefer the hexdigest form (looking like\n          // '7cf530335b8547945f1a48880bc421b2'), replace the above line with:\n          resolve(spark.end());\n        }\n      };\n  \n      processChunk(0);\n    });\n  }"],"file":"utils.js"}